{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "075d6b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      568454 non-null  int64 \n",
      " 1   ProductId               568454 non-null  object\n",
      " 2   UserId                  568454 non-null  object\n",
      " 3   ProfileName             568438 non-null  object\n",
      " 4   HelpfulnessNumerator    568454 non-null  int64 \n",
      " 5   HelpfulnessDenominator  568454 non-null  int64 \n",
      " 6   Score                   568454 non-null  int64 \n",
      " 7   Time                    568454 non-null  int64 \n",
      " 8   Summary                 568427 non-null  object\n",
      " 9   Text                    568454 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 43.4+ MB\n",
      "None\n",
      "\n",
      "First 5 Rows:\n",
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the dataset (update with your file path)\n",
    "file_path = 'Reviews.csv'  # Replace with the path to your downloaded file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(data.info())\n",
    "\n",
    "print(\"\\nFirst 5 Rows:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ec8244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " Id                         0\n",
      "ProductId                  0\n",
      "UserId                     0\n",
      "ProfileName               16\n",
      "HelpfulnessNumerator       0\n",
      "HelpfulnessDenominator     0\n",
      "Score                      0\n",
      "Time                       0\n",
      "Summary                   27\n",
      "Text                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\\n\", data.isnull().sum())\n",
    "\n",
    "# Drop rows with missing UserId, ProductId, or Score\n",
    "data = data.dropna(subset=['UserId', 'ProductId', 'Score'])\n",
    "\n",
    "# Fill missing text columns with an empty string\n",
    "data['Summary'] = data['Summary'].fillna('')\n",
    "data['Text'] = data['Text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e26c9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convert 'Time' column to datetime\n",
    "data['ReviewTime'] = data['Time'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "\n",
    "# Extract year and month from the review time\n",
    "data['Year'] = data['ReviewTime'].dt.year\n",
    "data['Month'] = data['ReviewTime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1354a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data Shape: (447842, 13)\n"
     ]
    }
   ],
   "source": [
    "# Filter data for the years 2010 to 2012\n",
    "recent_data = data[data['Year'].isin([2010, 2011, 2012])]\n",
    "\n",
    "print(\"Filtered Data Shape:\", recent_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b33125b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Shape: (10000, 13)\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 10,000 rows if the filtered data is still large\n",
    "subset_data = recent_data.sample(n=10000, random_state=42)\n",
    "\n",
    "print(\"Subset Shape:\", subset_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18fd4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Products for User: Index(['B0000E5JR0', 'B0000E65WO', 'B0000DJDJZ', 'B0000DK4G4', 'B0000DIYKE',\n",
      "       'B0000DID60', 'B0000D9MYO', 'B0000DJ4BQ', 'B0000CER9K', 'B0000D9MTS'],\n",
      "      dtype='object', name='ProductId')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "# Apply SVD to the interaction matrix\n",
    "interaction_matrix = subset_data.pivot_table(\n",
    "    index='UserId',\n",
    "    columns='ProductId',\n",
    "    values='Score'\n",
    ").fillna(0)\n",
    "\n",
    "# Convert the matrix to a NumPy array\n",
    "interaction_matrix_np = interaction_matrix.values\n",
    "\n",
    "# Apply SVD\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "svd_matrix = svd.fit_transform(interaction_matrix_np)\n",
    "\n",
    "# Function to recommend top N products for a user\n",
    "def recommend_products(user_index, svd_matrix, interaction_matrix, top_n=10):\n",
    "    user_ratings = svd_matrix[user_index]\n",
    "    product_indices = np.argsort(-user_ratings)[:top_n]  # Get top N indices\n",
    "    recommended_products = interaction_matrix.columns[product_indices]\n",
    "    return recommended_products\n",
    "\n",
    "# Example: Recommend products for the first user in the matrix\n",
    "user_index = 0  # Adjust as needed\n",
    "recommended_products = recommend_products(user_index, svd_matrix, interaction_matrix, top_n=10)\n",
    "print(\"Recommended Products for User:\", recommended_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f2a1cb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c4429fb415e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Compute cosine similarity between products\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Function to recommend similar products\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity between products\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Function to recommend similar products\n",
    "def recommend_similar_products(product_index, cosine_sim, product_ids, top_n=10):\n",
    "    similar_indices = np.argsort(-cosine_sim[product_index])[:top_n]  # Get top N indices\n",
    "    similar_products = [product_ids[i] for i in similar_indices]\n",
    "    return similar_products\n",
    "\n",
    "# Example: Recommend products similar to the first product in the dataset\n",
    "product_index = 0  # Adjust as needed\n",
    "product_ids = subset_data['ProductId'].unique()\n",
    "similar_products = recommend_similar_products(product_index, cosine_sim, product_ids, top_n=10)\n",
    "print(\"Similar Products to Product:\", product_ids[product_index])\n",
    "print(similar_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa9cf3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (10000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine text data to create a richer product description\n",
    "subset_data['combined_text'] = subset_data['Summary'] + \" \" + subset_data['Text']\n",
    "\n",
    "# Create a TF-IDF vectorizer and fit-transform the combined text\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(subset_data['combined_text'])\n",
    "\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d27cfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity computed successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity using the TF-IDF matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix, dense_output=False)\n",
    "\n",
    "print(\"Cosine similarity computed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f633a678",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7854 is out of bounds for axis 0 with size 6497",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-706c13665a5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mproduct_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# Replace with the index of a product in your dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mproduct_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubset_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ProductId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Unique product IDs from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msimilar_products\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommend_similar_products\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Products similar to product {product_ids[product_index]}: {similar_products}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-706c13665a5f>\u001b[0m in \u001b[0;36mrecommend_similar_products\u001b[0;34m(product_index, cosine_sim, product_ids, top_n)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Get product IDs of similar products\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msimilar_products\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mproduct_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimilar_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msimilar_products\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-706c13665a5f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Get product IDs of similar products\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msimilar_products\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mproduct_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimilar_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msimilar_products\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 7854 is out of bounds for axis 0 with size 6497"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to recommend top N similar products\n",
    "def recommend_similar_products(product_index, cosine_sim, product_ids, top_n=10):\n",
    "    # Get similarity scores for the given product\n",
    "    similarity_scores = cosine_sim[product_index].toarray().flatten()\n",
    "    \n",
    "    # Get indices of the top N similar products (excluding itself)\n",
    "    similar_indices = similarity_scores.argsort()[-top_n-1:-1][::-1]\n",
    "    \n",
    "    # Get product IDs of similar products\n",
    "    similar_products = [product_ids[i] for i in similar_indices]\n",
    "    \n",
    "    return similar_products\n",
    "\n",
    "# Example usage\n",
    "product_index = 0  # Replace with the index of a product in your dataset\n",
    "product_ids = subset_data['ProductId'].unique()  # Unique product IDs from the dataset\n",
    "similar_products = recommend_similar_products(product_index, cosine_sim, product_ids, top_n=10)\n",
    "\n",
    "print(f\"Products similar to product {product_ids[product_index]}: {similar_products}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7620c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated function to recommend top N similar products\n",
    "def recommend_similar_products(product_index, cosine_sim, product_ids, top_n=10):\n",
    "    # Get similarity scores for the given product\n",
    "    similarity_scores = cosine_sim[product_index].toarray().flatten()\n",
    "    \n",
    "    # Exclude the product itself and get indices of the top N similar products\n",
    "    similar_indices = np.argsort(-similarity_scores)[1:top_n+1]\n",
    "    \n",
    "    # Map the indices to product IDs\n",
    "    similar_products = product_ids[similar_indices]\n",
    "    \n",
    "    return similar_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d5f314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm product IDs correspond to the rows in tfidf_matrix\n",
    "product_ids = subset_data['ProductId'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04ccbdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products similar to product B0029NS7BU: 7854    B000F9Z29U\n",
      "7624    B003QNJYXM\n",
      "7851    B00451ZJB0\n",
      "4001    B0033GZMXS\n",
      "1739    B000WFRUP6\n",
      "3652    B005HGAV0G\n",
      "8567    B000YSS7EO\n",
      "9495    B003VXHGPK\n",
      "2027    B000ILEITA\n",
      "8594    B000MXGMIE\n",
      "Name: ProductId, dtype: object\n"
     ]
    }
   ],
   "source": [
    "product_index = 0  # Replace with a valid index in your dataset\n",
    "similar_products = recommend_similar_products(product_index, cosine_sim, product_ids, top_n=10)\n",
    "\n",
    "print(f\"Products similar to product {product_ids[product_index]}: {similar_products}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d83661c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@3: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Example: Assuming we have a list of actual purchased items per user for evaluation\n",
    "actual_items = {\n",
    "    \"User1\": [\"ProductA\", \"ProductB\"],\n",
    "    \"User2\": [\"ProductC\"],\n",
    "    # Add more users and their purchased products for evaluation\n",
    "}\n",
    "\n",
    "# Function to calculate Precision@K\n",
    "def precision_at_k(recommended, actual, k):\n",
    "    recommended_at_k = recommended[:k]\n",
    "    relevant_and_recommended = set(recommended_at_k).intersection(set(actual))\n",
    "    precision = len(relevant_and_recommended) / k\n",
    "    return precision\n",
    "\n",
    "# Example usage\n",
    "recommended_items = [\"ProductA\", \"ProductD\", \"ProductE\"]  # Top K recommended items\n",
    "actual_items_user1 = actual_items[\"User1\"]\n",
    "precision = precision_at_k(recommended_items, actual_items_user1, k=3)\n",
    "print(f\"Precision@3: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaa082e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "\u001b[K     |████████████████████████████████| 255 kB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub>=0.20.0\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "\u001b[K     |████████████████████████████████| 450 kB 49.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.11.0 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scipy in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: Pillow in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (8.2.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.24.1)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.0 MB 120.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (4.59.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.5.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "\u001b[K     |████████████████████████████████| 179 kB 58.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (2.4.7)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (2.11.3)\n",
      "Requirement already satisfied: networkx in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (2.5)\n",
      "Requirement already satisfied: sympy in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (1.8)\n",
      "Collecting tokenizers<0.21,>=0.20\n",
      "  Downloading tokenizers-0.20.3-cp38-cp38-macosx_10_12_x86_64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 26.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2021.4.4)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.5-cp38-cp38-macosx_10_12_x86_64.whl (392 kB)\n",
      "\u001b[K     |████████████████████████████████| 392 kB 33.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from networkx->torch>=1.11.0->sentence-transformers) (5.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2020.12.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.2.1)\n",
      "Installing collected packages: typing-extensions, fsspec, huggingface-hub, tokenizers, safetensors, transformers, sentence-transformers\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.5.0\n",
      "    Uninstalling typing-extensions-4.5.0:\n",
      "      Successfully uninstalled typing-extensions-4.5.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.9.0\n",
      "    Uninstalling fsspec-0.9.0:\n",
      "      Successfully uninstalled fsspec-0.9.0\n",
      "Successfully installed fsspec-2024.10.0 huggingface-hub-0.27.0 safetensors-0.4.5 sentence-transformers-3.2.1 tokenizers-0.20.3 transformers-4.46.3 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a60cbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vanshdeepmadan/opt/anaconda3/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5086bc7fa96d42068301accb04eec2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb11770e42ab465c8bf1e94922cbbe44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e40b8ac215043499ca24b8bc6a8cecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc5ddd1b9b3498890c30374779953c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db46a4970b84430b0cdc263152e12f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50c281fc07c40e5ae9b6597b4e84aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73567ba6b6140f7b96e88c07dcfe177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6b5a636be94009994a4599ac63400b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c71bcc5b94842a08a876a71c80497bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c045ee7567344049c627705f917965a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ef0d68e2f2481986c417e8514a5c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b37e3a38114d0abbf600a9bcca9a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10000, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Combine text data for richer descriptions (if not done already)\n",
    "subset_data['combined_text'] = subset_data['Summary'].fillna('') + \" \" + subset_data['Text'].fillna('')\n",
    "\n",
    "# Generate embeddings for the combined text\n",
    "embeddings = model.encode(subset_data['combined_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9deb73a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity matrix computed!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity between embeddings\n",
    "cosine_sim = cosine_similarity(embeddings, embeddings)\n",
    "\n",
    "print(\"Cosine similarity matrix computed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e9e6d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products similar to product B0029NS7BU: ['B003VMY488', 'B001E5116C', 'B001BOVDNC', 'B0081XPTBS', 'B004QQ82L8', 'B001BDEI6W', 'B0009X0RA6', 'B001E6IUMY', 'B001M08YZA', 'B005CUU23S']\n"
     ]
    }
   ],
   "source": [
    "# Function to recommend top N similar products using embeddings\n",
    "def recommend_similar_products(product_index, cosine_sim, product_ids, top_n=10):\n",
    "    similarity_scores = cosine_sim[product_index]\n",
    "    similar_indices = similarity_scores.argsort()[-top_n-1:-1][::-1]  # Top N similar products\n",
    "    similar_products = [product_ids[i] for i in similar_indices]\n",
    "    return similar_products\n",
    "\n",
    "# Example usage\n",
    "product_index = 0  # Replace with a valid product index\n",
    "product_ids = subset_data['ProductId'].reset_index(drop=True)  # Ensure alignment\n",
    "similar_products = recommend_similar_products(product_index, cosine_sim, product_ids, top_n=10)\n",
    "\n",
    "print(f\"Products similar to product {product_ids[product_index]}: {similar_products}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ebe10c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Precision@3: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Precision@3 for a sample user\n",
    "recommended_items = recommend_similar_products(0, cosine_sim, product_ids, top_n=3)\n",
    "actual_items_user1 = actual_items[\"User1\"]  # Replace with your actual items\n",
    "precision = precision_at_k(recommended_items, actual_items_user1, k=3)\n",
    "print(f\"Improved Precision@3: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dab0e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset data saved successfully to subset_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path where you want to save the subset data\n",
    "output_file_path = \"subset_data.csv\"\n",
    "\n",
    "# Save the subset data to a CSV file\n",
    "subset_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Subset data saved successfully to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921abd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
